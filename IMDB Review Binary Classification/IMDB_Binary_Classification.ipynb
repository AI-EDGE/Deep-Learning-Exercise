{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Binary_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4XN2BbwdSx2RvJhh5xuon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebinefrancis/Deep-Learning-Exercise/blob/IMDB-Movie-Review-Binary-Classification/IMDB%20Review%20Binary%20Classification/IMDB_Binary_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSfHKzIaR7Ow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02b12365-0ab3-42b0-c3e8-0f4a4cd9a2a0"
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3TgV518aDmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWfZxqNMawil",
        "colab_type": "text"
      },
      "source": [
        "Because you’re restricting yourself to the top 10,000 most frequent words, no word\n",
        "**index** will exceed 10,000:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQdp-LSacAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc1fbbc9-d512-4c9c-9330-34bcb334b9f5"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKlCfrWvaghp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQbMJcjgakMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c874f858-305f-4a77-9656-8f96bca0a1be"
      },
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ntpAobGchqt",
        "colab_type": "text"
      },
      "source": [
        "Because you’re restricting yourself to the top 10,000 most frequent words, no word\n",
        "**index** will exceed 10,000: Here we check for highest number that appear in each review (ie highest index number that appear in each sequence in `train_data`)and then find highest of those highest indices of each review\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v2-AOhPdAkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3 , '?') for i in train_data[0]])\n",
        "print(decoded_review)\n",
        "#print(reverse_word_index.get(0,'?'))\n",
        "#print(reverse_word_index.get(14,'?'))\n",
        "#print(reverse_word_index.get(11,'?'))\n",
        "#train_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpJbzItk_Vi",
        "colab_type": "text"
      },
      "source": [
        "The integers represnting indices of words in review are offset by 3 from that of dictionary. Because in review it is considered that 0 = \"PADDING\" 1=\"START\" and 2=\"UNKNOWN\". If there is 14 in review, it means 14-3 = 11 is the correct word in dictionary. so 14 in review represents 11 in dictionary. 3 in review repsents 0 in dictionary. 4=1, 5=2 AND SO ON. If i = 2 (means \"UNKNOWN\" in review), 2-3=-1, which is not a valid index for dictionary and our code will place '?' there. So for 0,1,2 in reviw we get a '?'\n",
        "\n",
        "[Explanation](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset/44891281)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZq6yt-knHIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# results = np.zeros((2, 5))\n",
        "# results[1, [1,3]] = 1.\n",
        "# print(results)\n",
        "# results = np.zeros((2, 5))\n",
        "# results[1, [1,1]] = 1. # [1,1] is similar to same word repeated 2 times in review\n",
        "# print(results)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension)) #creates all zero matrix, \n",
        "  #with dimensions = 10000 in one axis and equal to number of reviews in other axis\n",
        "  for i, sequence in enumerate(sequences): #iterate each reviews and corresponding result axis\n",
        "    results[i, sequence] = 1. #set for each result (i th result), the values in indices\n",
        "    #corresponding to the sequence(review) value, to 1. So if first review has first value 3,\n",
        "    # then 3rd indice in first result is 1. So basically this result indicate whether that review\n",
        "    #contains the word or not (atleast once). Uncomment the code to see it.\n",
        "    #Sets specific indices of results[i] to 1s\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "x_train[0]\n",
        "\n",
        "#Now vectorize our labels as well. Just changing data type, and convert to tensor\n",
        "# our labels are already 0 and 1s.\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myoAn_lJwl_C",
        "colab_type": "text"
      },
      "source": [
        "Now the data is ready to fed into network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u0C3IKrtBfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e03c42a-9504-4829-ce57-a969e50a9439"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}